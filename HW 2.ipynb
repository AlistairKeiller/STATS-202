{
  "cells": [
    {
      "cell_type": "raw",
      "id": "1832903c",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'STATS 202 Homework 2'\n",
        "format:\n",
        "  html:\n",
        "      code-fold: true\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "646fd649",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.formula.api as smf\n",
        "from IPython.display import Latex, Markdown\n",
        "from ISLP import load_data\n",
        "from sklearn.discriminant_analysis import (\n",
        "    LinearDiscriminantAnalysis,\n",
        "    QuadraticDiscriminantAnalysis,\n",
        ")\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "weekly = load_data(\"Weekly\")\n",
        "default = load_data(\"default\")\n",
        "sns.set_theme(style=\"dark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adcf12aa",
      "metadata": {},
      "source": [
        "# Problem 1\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "p(X)&=\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}} \\\\\n",
        "\\frac{p(X)}{1-p(X)}&=\\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n",
        "&=\\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n",
        "&=\\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n",
        "&=\\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n",
        "&=\\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n",
        "&=e^{\\beta_0+\\beta_1 X}\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "# Problem 2\n",
        "\n",
        "## Problem 2.a\n",
        "\n",
        "On average, $\\frac{1}{10}$ of the observations will be used to make the predictions.\n",
        "\n",
        "## Problem 2.b\n",
        "\n",
        "On average, $\\frac{1}{100}$ of the observations will be used to make the predictions.\n",
        "\n",
        "## Problem 2.c\n",
        "\n",
        "On average, $10^{-100}$ of the observations will be used to be used to make the predictions.\n",
        "\n",
        "## Problem 2.d\n",
        "\n",
        "As the number of features increase, fewer of the larger set will be used for inference, which will make the model seem relatively 'dumber' as it sees a very local pictures and never really 'understanding' a larger picture, therefore also causing really high variance and overfitting.\n",
        "\n",
        "## Problem 2.e\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "s^{p}&=10^{-1} \\\\\n",
        "p\\ln s&=-\\ln 10 \\\\\n",
        "\\ln s&=-p^{-1}\\ln 10 \\\\\n",
        "s&=10^{-p^{-1}}\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "### p=1\n",
        "\n",
        "$$\n",
        "s=10^{-1}=0.1\n",
        "$$\n",
        "\n",
        "### p=2\n",
        "\n",
        "$$\n",
        "s=10^{-0.5}\\approx .32\n",
        "$$\n",
        "\n",
        "### p=100\n",
        "\n",
        "$$\n",
        "s=10^{-.01}\\approx 0.98\n",
        "$$\n",
        "\n",
        "# Problem 3\n",
        "\n",
        "## Problem 3.a\n",
        "\n",
        "$$\n",
        "Y=\\sigma\\left(\\hat{\\beta}_0+\\hat{\\beta}_1 X_1+\\hat{\\beta}_2 X_2\\right)=\\sigma\\left(-6+40\\cdot .05+1\\cdot 3.5\\right)=\\sigma(-0.5)\\approx .377\n",
        "$$\n",
        "\n",
        "## Problem 3.b\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "0.5&=\\sigma\\left(\\hat{\\beta}_0+\\hat{\\beta}_1 X_1+\\hat{\\beta}_2 X_2\\right)=\\sigma\\left(-6+0.05h+3.5\\right) \\\\\n",
        "0.5&=\\sigma\\left(0.05h-2.5\\right)\n",
        "h\\approx 50\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "# Problem 4\n",
        "\n",
        "We would prefer the regression, because the $K=1$ KNN will have a $0\\%$ error on the training data ( the NN of any data point in the training data set will just be itself ), which means that it had a $36\\%$ error on the testing data. Scince the $36\\%$ test error for KNN is more than the $30\\%$ error on the linear regression, we would prefer the regression to classify new observations.\n",
        "\n",
        "# Problem 5\n",
        "\n",
        "## Problem 5.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3bd095d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | warning: false\n",
        "sns.pairplot(weekly, hue=\"Direction\", diag_kws={\"multiple\": \"stack\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0856a6e",
      "metadata": {},
      "source": [
        "From looking at the graphs, we can see that Today is clustered by Direction and there is a correlation between Volume and year.\n",
        "\n",
        "## Problem 5.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb9ac3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "weekly_binary = weekly.assign(Direction=(weekly[\"Direction\"] == \"Up\").astype(int))\n",
        "model = smf.logit(\n",
        "    \"Direction ~ Volume + Lag1 + Lag2 + Lag3 + Lag4 + Lag5\", data=weekly_binary\n",
        ").fit(disp=False)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4941132",
      "metadata": {},
      "source": [
        "Lag2 is the only statistically significant correlation.\n",
        "\n",
        "## Problem 5.c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea560adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = model.predict(\n",
        "    weekly_binary[[\"Volume\", \"Lag1\", \"Lag2\", \"Lag3\", \"Lag4\", \"Lag5\"]]\n",
        ").round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5b97a2",
      "metadata": {},
      "source": [
        "We can see that the model is very hesitant to guess 0, so it has a very low recall,\n",
        "\n",
        "## Problem 5.d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b5dd3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "weekly_training = weekly[weekly[\"Year\"] <= 2008]\n",
        "weekly_training_binary = weekly_training.assign(\n",
        "    Direction=(weekly_training[\"Direction\"] == \"Up\").astype(int)\n",
        ")\n",
        "weekly_testing = weekly[weekly[\"Year\"] >= 2009]\n",
        "weekly_testing_binary = weekly_testing.assign(\n",
        "    Direction=(weekly_testing[\"Direction\"] == \"Up\").astype(int)\n",
        ")\n",
        "model = smf.logit(\"Direction ~ Lag2\", data=weekly_training_binary).fit(disp=False)\n",
        "prediction = model.predict(weekly_testing_binary[[\"Lag2\"]]).round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_testing_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bbb6c0",
      "metadata": {},
      "source": [
        "## Probelm 5.e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bebb60f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LinearDiscriminantAnalysis().fit(\n",
        "    weekly_training_binary[[\"Lag2\"]], weekly_training_binary[\"Direction\"]\n",
        ")\n",
        "prediction = model.predict(weekly_testing_binary[[\"Lag2\"]]).round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_testing_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b793cd5",
      "metadata": {},
      "source": [
        "## Problem 5.f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c7e0bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = QuadraticDiscriminantAnalysis().fit(\n",
        "    weekly_training_binary[[\"Lag2\"]], weekly_training_binary[\"Direction\"]\n",
        ")\n",
        "prediction = model.predict(weekly_testing_binary[[\"Lag2\"]]).round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_testing_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b9743b",
      "metadata": {},
      "source": [
        "## Problem 5.g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a2a803",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = KNeighborsClassifier(n_neighbors=1).fit(\n",
        "    weekly_training_binary[[\"Lag2\"]], weekly_training_binary[\"Direction\"]\n",
        ")\n",
        "prediction = model.predict(weekly_testing_binary[[\"Lag2\"]]).round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_testing_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7934fc26",
      "metadata": {},
      "source": [
        "## Probem 5.h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f48ce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GaussianNB().fit(\n",
        "    weekly_training_binary[[\"Lag2\"]], weekly_training_binary[\"Direction\"]\n",
        ")\n",
        "prediction = model.predict(weekly_testing_binary[[\"Lag2\"]]).round()\n",
        "ConfusionMatrixDisplay.from_predictions(weekly_testing_binary[\"Direction\"], prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298f3e64",
      "metadata": {},
      "source": [
        "# Problem 6\n",
        "\n",
        "## Problem 6.a\n",
        "\n",
        "Each bootstrap observation has a $\\frac{1}{n}$ chance of being equal to the $j$th element of the original sample, so it has a $1-\\frac{1}{n}=\\frac{n-1}{n}$ chance of not being the $j$th observation.\n",
        "\n",
        "## Problem 6.b\n",
        "\n",
        "Every bootstrap observation is sampled identically, so it is also $\\frac{n-1}{n}$\n",
        "\n",
        "## Problem 6.c\n",
        "\n",
        "Each bootstrap observation has a $1-\\frac{1}{n}$ probability of not being the $j$th observation, so the chance that every one of $n$ bootsrap observations in a bootstrap sample of size $n$ is not the $j$th observation is $\\left(1-\\frac{1}{n}\\right)^n$.\n",
        "\n",
        "## Problem 6.d\n",
        "\n",
        "$\\left(1-\\frac{1}{n}\\right)^n=\\left(1-\\frac{1}{5}\\right)^5\\approx 0.33$\n",
        "\n",
        "## Problem 6.e\n",
        "\n",
        "$\\left(1-\\frac{1}{n}\\right)^n=\\left(1-\\frac{1}{100}\\right)^{100}\\approx 0.37$\n",
        "\n",
        "## Problem 6.f\n",
        "\n",
        "$\\left(1-\\frac{1}{n}\\right)^n=\\left(1-\\frac{1}{10000}\\right)^{10000}\\approx 0.37$\n",
        "\n",
        "## Problem 6.g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0c7289",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.arange(1, 10000)\n",
        "sns.lineplot((1 - 1 / x) ** x)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5e517c",
      "metadata": {},
      "source": [
        "$\\left(1-\\frac{1}{n}\\right)^n$ increases and asymptotes to $e^{-1}$.\n",
        "\n",
        "## Python 6.h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def4e16",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(10)\n",
        "store = np.empty(10000)\n",
        "for i in range(10000):\n",
        "    store[i] = np.sum(rng.choice(100, replace=True) == 4) > 0\n",
        "np.mean(store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be6e768",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "np.mean(np.random.randint(1, 100, 10000) == 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be653541",
      "metadata": {},
      "source": [
        "These are both very different from the estimate of $\\left(1-\\frac{1}{100}\\right)^{100}=.37$; the python ones are significantly lower and the R is is significantly higher.\n",
        "\n",
        "# Problem 7\n",
        "\n",
        "## Probblem 7.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b43d55b",
      "metadata": {},
      "outputs": [],
      "source": [
        "default_binary = default.assign(\n",
        "    default=(default[\"default\"] == \"Yes\").astype(int),\n",
        "    student=(default[\"student\"] == \"Yes\").astype(int),\n",
        ")\n",
        "model = smf.logit(\"default ~ income + balance\", data=default_binary).fit(disp=False)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b7f4ef",
      "metadata": {},
      "source": [
        "## Problem 7.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a13baec",
      "metadata": {},
      "outputs": [],
      "source": [
        "default_binary_train, default_binary_test = train_test_split(\n",
        "    default_binary, test_size=0.5, random_state=42\n",
        ")\n",
        "model = smf.logit(\"default ~ income + balance\", data=default_binary_train).fit(\n",
        "    disp=False\n",
        ")\n",
        "prediction = model.predict(default_binary_test[[\"income\", \"balance\"]]).round()\n",
        "error = sum(prediction != default_binary_test[\"default\"]) / len(\n",
        "    default_binary_test[\"default\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ac4d82",
      "metadata": {},
      "source": [
        "The error is `{python} error`\n",
        "\n",
        "## Problem 7.c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e65eb6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = [0.25, 0.5, 0.75]\n",
        "errors = []\n",
        "for split in splits:\n",
        "    default_binary_train, default_binary_test = train_test_split(\n",
        "        default_binary, test_size=split, random_state=42\n",
        "    )\n",
        "    model = smf.logit(\"default ~ income + balance\", data=default_binary_train).fit(\n",
        "        disp=False\n",
        "    )\n",
        "    prediction = model.predict(default_binary_test[[\"income\", \"balance\"]]).round()\n",
        "    error = sum(\n",
        "        prediction\n",
        "        != (default_binary_test[\"default\"]) / len(default_binary_test[\"default\"])\n",
        "    )\n",
        "    errors.append(error)\n",
        "Latex(pd.DataFrame({\"Training Split\": splits, \"Error\": errors}).to_latex())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b314b18d",
      "metadata": {},
      "source": [
        "From these there seems to be a almost parabolic motion centered somewhere around $0.5$, such that increasing or decreasing the training split will derease performance on the testing data.\n",
        "\n",
        "## Problem 7.d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43e3e4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "default_binary_train, default_binary_test = train_test_split(\n",
        "    default_binary, test_size=0.5, random_state=42\n",
        ")\n",
        "model = smf.logit(\n",
        "    \"default ~ student + income + balance\", data=default_binary_train\n",
        ").fit(disp=False)\n",
        "prediction = model.predict(\n",
        "    default_binary_test[[\"student\", \"income\", \"balance\"]]\n",
        ").round()\n",
        "error = sum(prediction != default_binary_test[\"default\"]) / len(\n",
        "    default_binary_test[\"default\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb0cb5c",
      "metadata": {},
      "source": [
        "The error is `{python} error`, which is an improvment over the version without student, which means that student is providing more information to the model rather than just providing a point of overfitting for the training.\n",
        "\n",
        "# Problem 8"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
