---
title: "STATS 202 Homework 2"
format:
  html:
      code-fold: true
  pdf:
    fig-width: 7
    fig-height: 7
---

# Problem 1

$$
\begin{split}
p(X)&=\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}} \\
\frac{p(X)}{1-p(X)}&=\frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{1-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
&=\frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1+e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
&=\frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1+e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
&=\frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1+e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
&=\frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1}{1+e^{\beta_0+\beta_1 X}}} \\
&=e^{\beta_0+\beta_1 X}
\end{split}
$$

# Problem 2

## Problem 2.a

On average, $\frac{1}{10}$ of the observations will be used to make the predictions.

## Problem 2.b

On average, $\frac{1}{100}$ of the observations will be used to make the predictions.

## Problem 2.c

On average, $10^{-100}$ of the observations will be used to be used to make the predictions.

## Problem 2.d

As the number of features increase, fewer of the larger set will be used for inference, which will make the model seem relatively "dumber" as it sees a very local pictures and never really "understanding" a larger picture, therefore also causing really high variance and overfitting.

## **Problem 2.e**

CHECK THAT THIS IS THE CORRECT INTERPRETATION

$$
\begin{split}
s^{p}&=10^{-1} \\
p\ln s&=-\ln 10 \\
\ln s&=-p^{-1}\ln 10 \\
s&=10^{-p^{-1}}
\end{split}
$$

### p=1

$$
s=10^{-1}=0.1
$$

### p=2

$$
s=10^{-0.5}\approx .32
$$

### p=100

$$
s=10^{-.01}\approx 0.98
$$

# Problem 3

## Problem 3.a

$$
Y=\sigma\left(\hat{\beta}_0+\hat{\beta}_1 X_1+\hat{\beta}_2 X_2\right)=\sigma\left(-6+40\cdot .05+1\cdot 3.5\right)=\sigma(-0.5)\approx .377
$$

## Problem 3.b

$$
\begin{split}
0.5&=\sigma\left(\hat{\beta}_0+\hat{\beta}_1 X_1+\hat{\beta}_2 X_2\right)=\sigma\left(-6+0.05h+3.5\right) \\
0.5&=\sigma\left(0.05h-2.5\right)
h\approx 50
\end{split}
$$

# Problem 4

We would prefer the regression, because the $K=1$ KNN will have a $0\%$ error on the training data ( the NN of any data point in the training data set will just be itself ), which means that it had a $36\%$ error on the testing data. Scince the $36\%$ test error for KNN is more than the $30\%$ error on the linear regression, we would prefer the regression to classify new observations.

# Problem 5

## Problem 5.a

```{r}
library(ISLR2)
plot(Weekly)
```

## Problem 5.b

```{r}

```